from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Optional
import logging
import os
from ..youtube import YouTubeClient
from ..scheduler import TaskScheduler
from ..storage import CSVGenerator
from ..config import settings
import json
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)

router = APIRouter()
# U≈ºywamy globalnej instancji z main.py
task_scheduler = None

def set_task_scheduler(scheduler_instance):
    """Ustawia globalnƒÖ instancjƒô schedulera"""
    global task_scheduler
    task_scheduler = scheduler_instance


# Pydantic models
class ChannelRequest(BaseModel):
    url: str
    category: str = "general"


class ChannelResponse(BaseModel):
    id: str
    title: str
    description: str
    subscriber_count: int
    video_count: int
    view_count: int
    thumbnail: str
    category: str


class ReportRequest(BaseModel):
    category: Optional[str] = None
    days_back: int = 3


class StatusResponse(BaseModel):
    scheduler_running: bool
    channels_count: int
    categories: List[str]
    quota_usage: Dict
    next_report: Optional[str]
    cache_stats: Optional[Dict] = None


class CategoryRequest(BaseModel):
    name: str


class CategoryResponse(BaseModel):
    name: str
    channels_count: int
    message: str


class CategoryInfo(BaseModel):
    name: str
    channels_count: int
    channels: List[Dict]
    has_reports: bool


@router.post("/channels", response_model=ChannelResponse)
async def add_channel(channel_request: ChannelRequest):
    """Dodaje kana≈Ç YouTube do monitorowania"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
            
        # Dodaj kana≈Ç przez scheduler
        channel_info = await task_scheduler.add_channel(channel_request.url, channel_request.category)
        
        # Dodaj kategoriƒô do odpowiedzi
        channel_info['category'] = channel_request.category
        
        logger.info(f"Dodano kana≈Ç: {channel_info['title']} do kategorii {channel_request.category}")
        return channel_info
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas dodawania kana≈Çu: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/channels", response_model=Dict[str, List[ChannelResponse]])
async def get_channels():
    """Zwraca listƒô wszystkich kana≈Ç√≥w"""
    try:
        if not task_scheduler:
            return {}
        channels = task_scheduler.get_channels()
        return channels
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania kana≈Ç√≥w: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/channels/{channel_id}")
async def remove_channel(channel_id: str, category: str = "general"):
    """Usuwa kana≈Ç z monitorowania"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        task_scheduler.remove_channel(channel_id, category)
        return {"message": f"Kana≈Ç {channel_id} zosta≈Ç usuniƒôty z kategorii {category}"}
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas usuwania kana≈Çu: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/categories", response_model=CategoryResponse)
async def add_category(category_request: CategoryRequest):
    """Dodaje nowƒÖ kategoriƒô"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        result = task_scheduler.add_category(category_request.name)
        logger.info(f"Dodano kategoriƒô: {category_request.name}")
        return result
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas dodawania kategorii: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/categories/{category_name}")
async def remove_category(category_name: str, force: bool = False):
    """Usuwa kategoriƒô (z opcjƒÖ force dla usuniƒôcia z kana≈Çami)"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        result = task_scheduler.remove_category(category_name, force)
        logger.info(f"Usuniƒôto kategoriƒô: {category_name} (force={force})")
        return result
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas usuwania kategorii: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/categories", response_model=List[CategoryInfo])
async def get_categories():
    """Zwraca listƒô wszystkich kategorii z liczbƒÖ kana≈Ç√≥w"""
    try:
        if not task_scheduler:
            return []
        
        categories = task_scheduler.get_categories()
        return categories
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania kategorii: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/reports/generate")
async def generate_report(report_request: ReportRequest):
    """Generuje raport CSV dla okre≈õlonej kategorii"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
            
        channels = task_scheduler.get_channels()
        
        if report_request.category and report_request.category not in channels:
            raise HTTPException(status_code=404, detail=f"Kategoria {report_request.category} nie istnieje")
        
        all_videos = {}
        
        # Pobierz dane z kana≈Ç√≥w
        target_categories = [report_request.category] if report_request.category else channels.keys()
        
        for category in target_categories:
            if category in channels:
                category_videos = []
                
                for channel in channels[category]:
                    try:
                        videos = await task_scheduler.get_channel_videos(
                            channel['id'], 
                            report_request.days_back
                        )
                        
                        # Dodaj informacje o kanale
                        for video in videos:
                            video['channel_title'] = channel['title']
                            video['channel_id'] = channel['id']
                        
                        category_videos.extend(videos)
                        
                    except Exception as e:
                        logger.error(f"B≈ÇƒÖd podczas pobierania film√≥w z kana≈Çu {channel['title']}: {e}")
                
                if category_videos:
                    all_videos[category] = category_videos
        
        if not all_videos:
            raise HTTPException(status_code=404, detail="Brak danych do wygenerowania raportu")
        
        # Generuj CSV
        csv_generator = CSVGenerator()
        
        if report_request.category:
            # Raport dla jednej kategorii
            csv_path = csv_generator.generate_csv(all_videos[report_request.category], report_request.category)
        else:
            # Raport podsumowujƒÖcy
            csv_path = csv_generator.generate_summary_csv(all_videos)
        
        return FileResponse(
            path=csv_path,
            filename=csv_path.split('/')[-1],
            media_type='text/csv'
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas generowania raportu: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports/list")
async def list_reports():
    """Zwraca listƒô dostƒôpnych raport√≥w"""
    try:
        import os
        from datetime import datetime
        
        reports = []
        reports_dir = settings.reports_path
        
        print(f"üìÇ Szukanie raport√≥w w: {reports_dir.absolute()}")
        logger.info(f"Szukanie raport√≥w w: {reports_dir.absolute()}")
        
        # Sprawd≈∫ czy katalog istnieje
        if not reports_dir.exists():
            print(f"‚ö†Ô∏è Katalog raport√≥w nie istnieje: {reports_dir.absolute()}")
            logger.warning(f"Katalog raport√≥w nie istnieje: {reports_dir.absolute()}")
            # Utw√≥rz katalog
            reports_dir.mkdir(parents=True, exist_ok=True)
            print(f"‚úÖ Utworzono katalog raport√≥w: {reports_dir.absolute()}")
            logger.info(f"Utworzono katalog raport√≥w: {reports_dir.absolute()}")
        
        # Listuj pliki CSV
        csv_files = list(reports_dir.glob("*.csv"))
        print(f"üìÑ Znaleziono {len(csv_files)} plik√≥w CSV")
        logger.info(f"Znaleziono {len(csv_files)} plik√≥w CSV")
        
        for file_path in csv_files:
            try:
                stats = os.stat(file_path)
                reports.append({
                    'filename': file_path.name,
                    'size': stats.st_size,
                    'created': stats.st_ctime,
                    'created_date': datetime.fromtimestamp(stats.st_ctime).isoformat(),
                    'path': str(file_path.absolute())
                })
                print(f"   üìÑ {file_path.name} ({stats.st_size} bytes)")
            except Exception as e:
                print(f"   ‚ùå B≈ÇƒÖd podczas czytania {file_path.name}: {e}")
                logger.error(f"B≈ÇƒÖd podczas czytania {file_path.name}: {e}")
        
        # Sortuj po dacie utworzenia (najnowsze pierwsze)
        sorted_reports = sorted(reports, key=lambda x: x['created'], reverse=True)
        
        print(f"‚úÖ Zwracam {len(sorted_reports)} raport√≥w")
        logger.info(f"Zwracam {len(sorted_reports)} raport√≥w")
        
        return {
            "reports": sorted_reports,
            "total_count": len(sorted_reports),
            "reports_directory": str(reports_dir.absolute())
        }
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas listowania raport√≥w: {e}")
        logger.error(f"B≈ÇƒÖd podczas listowania raport√≥w: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports/download/{filename}")
async def download_report(filename: str):
    """Pobiera konkretny raport"""
    try:
        file_path = settings.reports_path / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="Raport nie istnieje")
        
        return FileResponse(
            path=str(file_path),
            filename=filename,
            media_type='text/csv'
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania raportu: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status", response_model=StatusResponse)
async def get_status():
    """Zwraca status systemu"""
    try:
        if not task_scheduler:
            return StatusResponse(
                scheduler_running=False,
                channels_count=0,
                categories=[],
                quota_usage=task_scheduler.get_quota_usage(),
                next_report=None
            )
            
        scheduler_status = task_scheduler.get_status()
        quota_info = task_scheduler.get_quota_usage()
        
        # Pobierz statystyki cache
        cache_stats = task_scheduler.get_cache_stats()
        
        return StatusResponse(
            scheduler_running=scheduler_status['running'],
            channels_count=scheduler_status['channels_count'],
            categories=scheduler_status['categories'],
            quota_usage=quota_info,
            next_report=scheduler_status['next_run'],
            cache_stats=cache_stats
        )
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania statusu: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/scheduler/start")
async def start_scheduler():
    """Uruchamia scheduler"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        task_scheduler.start()
        return {"message": "Scheduler uruchomiony"}
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas uruchamiania schedulera: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/scheduler/stop")
async def stop_scheduler():
    """Zatrzymuje scheduler"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        task_scheduler.stop()
        return {"message": "Scheduler zatrzymany"}
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas zatrzymywania schedulera: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/cache/stats")
async def get_cache_stats():
    """Zwraca statystyki cache"""
    try:
        cache_stats = task_scheduler.get_cache_stats()
        return cache_stats
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania statystyk cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cache/cleanup")
async def cleanup_cache():
    """Czy≈õci przestarza≈Çy cache"""
    try:
        cleaned_count = task_scheduler.cleanup_cache()
        return {"message": f"Usuniƒôto {cleaned_count} przestarza≈Çych wpis√≥w z cache"}
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas czyszczenia cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/data/clear")
async def clear_all_data():
    """Czy≈õci wszystkie dane (dla test√≥w)"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        task_scheduler.state_manager.clear_all_data()
        return {"message": "Wszystkie dane zosta≈Çy wyczyszczone"}
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas czyszczenia danych: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/data/stats")
async def get_data_stats():
    """Zwraca statystyki danych"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        stats = task_scheduler.state_manager.get_data_stats()
        return stats
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas pobierania statystyk danych: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debug/json")
async def debug_json_files():
    """Debug endpoint - pokazuje zawarto≈õƒá plik√≥w JSON"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Sprawd≈∫ czy pliki istniejƒÖ
        channels_exists = state_manager.channels_file.exists()
        quota_exists = state_manager.quota_file.exists()
        system_exists = state_manager.system_state_file.exists()
        
        # Wczytaj zawarto≈õƒá plik√≥w
        channels_content = None
        quota_content = None
        system_content = None
        
        if channels_exists:
            try:
                with open(state_manager.channels_file, 'r', encoding='utf-8') as f:
                    channels_content = json.load(f)
            except Exception as e:
                channels_content = f"B≈ÇƒÖd odczytu: {e}"
        
        if quota_exists:
            try:
                with open(state_manager.quota_file, 'r', encoding='utf-8') as f:
                    quota_content = json.load(f)
            except Exception as e:
                quota_content = f"B≈ÇƒÖd odczytu: {e}"
        
        if system_exists:
            try:
                with open(state_manager.system_state_file, 'r', encoding='utf-8') as f:
                    system_content = json.load(f)
            except Exception as e:
                system_content = f"B≈ÇƒÖd odczytu: {e}"
        
        return {
            "data_directory": str(state_manager.data_dir.absolute()),
            "files": {
                "channels.json": {
                    "exists": channels_exists,
                    "path": str(state_manager.channels_file.absolute()),
                    "content": channels_content
                },
                "quota_state.json": {
                    "exists": quota_exists,
                    "path": str(state_manager.quota_file.absolute()),
                    "content": quota_content
                },
                "system_state.json": {
                    "exists": system_exists,
                    "path": str(state_manager.system_state_file.absolute()),
                    "content": system_content
                }
            },
            "memory_state": {
                "channels_count": sum(len(channels) for channels in state_manager.channels_data.values()),
                "quota_used": state_manager.quota_state.get('used', 0),
                "system_startup": state_manager.system_state.get('last_startup')
            }
        }
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania plik√≥w JSON: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/reports")
async def debug_reports():
    """Debug endpoint - pokazuje informacje o katalogu raport√≥w"""
    try:
        import os
        from datetime import datetime
        
        reports_dir = settings.reports_path
        
        # Sprawd≈∫ czy katalog istnieje
        exists = reports_dir.exists()
        absolute_path = str(reports_dir.absolute())
        
        # Sprawd≈∫ uprawnienia
        can_write = False
        can_read = False
        if exists:
            try:
                test_file = reports_dir / "test_write.tmp"
                test_file.write_text("test")
                test_file.unlink()
                can_write = True
            except Exception:
                can_write = False
            
            try:
                list(reports_dir.iterdir())
                can_read = True
            except Exception:
                can_read = False
        
        # Listuj pliki
        csv_files = []
        if exists and can_read:
            for file_path in reports_dir.glob("*.csv"):
                try:
                    stats = os.stat(file_path)
                    csv_files.append({
                        'name': file_path.name,
                        'size': stats.st_size,
                        'created': datetime.fromtimestamp(stats.st_ctime).isoformat(),
                        'modified': datetime.fromtimestamp(stats.st_mtime).isoformat(),
                        'path': str(file_path.absolute())
                    })
                except Exception as e:
                    csv_files.append({
                        'name': file_path.name,
                        'error': str(e)
                    })
        
        return {
            "reports_directory": {
                "path": absolute_path,
                "exists": exists,
                "can_write": can_write,
                "can_read": can_read
            },
            "csv_files": csv_files,
            "total_csv_files": len(csv_files),
            "railway_volume_path": os.getenv("RAILWAY_VOLUME_PATH", "Not set")
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania raport√≥w: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/env")
async def debug_environment():
    """Debug endpoint - pokazuje zmienne ≈õrodowiskowe"""
    try:
        import os
        
        env_vars = {
            "RAILWAY_VOLUME_PATH": os.getenv("RAILWAY_VOLUME_PATH", "Not set"),
            "RAILWAY_ENVIRONMENT": os.getenv("RAILWAY_ENVIRONMENT", "Not set"),
            "RAILWAY_PROJECT_ID": os.getenv("RAILWAY_PROJECT_ID", "Not set"),
            "RAILWAY_SERVICE_ID": os.getenv("RAILWAY_SERVICE_ID", "Not set"),
            "PWD": os.getcwd(),
            "HOME": os.getenv("HOME", "Not set"),
            "USER": os.getenv("USER", "Not set"),
        }
        
        return {
            "environment_variables": env_vars,
            "current_working_directory": os.getcwd(),
            "data_directory_exists": os.path.exists("/app/data"),
            "data_directory_writable": os.access("/app/data", os.W_OK) if os.path.exists("/app/data") else False,
            "reports_directory_exists": os.path.exists("/app/reports"),
            "reports_directory_writable": os.access("/app/reports", os.W_OK) if os.path.exists("/app/reports") else False,
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania zmiennych ≈õrodowiskowych: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.post("/debug/test-persistence")
async def test_data_persistence():
    """Test endpoint - sprawdza trwa≈Ço≈õƒá danych"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Dodaj testowy kana≈Ç
        test_channel = {
            "id": "TEST_CHANNEL_123",
            "title": "Test Channel",
            "description": "Test channel for persistence",
            "subscriber_count": 1000,
            "video_count": 50,
            "view_count": 100000,
            "thumbnail": "https://example.com/thumb.jpg",
            "published_at": "2020-01-01T00:00:00Z"
        }
        
        # Zapisz kana≈Ç
        state_manager.add_channel(test_channel, "test_persistence")
        
        # Sprawd≈∫ czy zosta≈Ç zapisany
        channels = state_manager.get_channels()
        test_channels = channels.get("test_persistence", [])
        
        # Sprawd≈∫ czy plik istnieje
        channels_file_exists = state_manager.channels_file.exists()
        
        return {
            "test_channel_added": len(test_channels) > 0,
            "test_channel_id": test_channels[0]["id"] if test_channels else None,
            "channels_file_exists": channels_file_exists,
            "channels_file_path": str(state_manager.channels_file.absolute()),
            "total_channels": sum(len(channels) for channels in channels.values()),
            "message": "Test kana≈Ç zosta≈Ç dodany. Sprawd≈∫ czy przetrwa restart Railway."
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas testowania trwa≈Ço≈õci danych: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/check-persistence")
async def check_data_persistence():
    """Sprawdza czy dane sƒÖ trwa≈Çe po restarcie"""
    try:
        import os
        
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Sprawd≈∫ czy pliki istniejƒÖ
        channels_exists = state_manager.channels_file.exists()
        quota_exists = state_manager.quota_file.exists()
        system_exists = state_manager.system_state_file.exists()
        
        # Sprawd≈∫ dane w pamiƒôci
        channels_count = sum(len(channels) for channels in state_manager.channels_data.values())
        quota_used = state_manager.quota_state.get('used', 0)
        system_startup = state_manager.system_state.get('last_startup')
        
        # Sprawd≈∫ czy sƒÖ testowe kana≈Çy
        test_channels = state_manager.channels_data.get("test_persistence", [])
        has_test_channel = any(channel.get("id") == "TEST_CHANNEL_123" for channel in test_channels)
        
        return {
            "files_exist": {
                "channels.json": channels_exists,
                "quota_state.json": quota_exists,
                "system_state.json": system_exists
            },
            "memory_data": {
                "channels_count": channels_count,
                "quota_used": quota_used,
                "system_startup": system_startup
            },
            "test_persistence": {
                "has_test_channel": has_test_channel,
                "test_channels_count": len(test_channels)
            },
            "data_directory": str(state_manager.data_dir.absolute()),
            "railway_volume_path": os.getenv("RAILWAY_VOLUME_PATH", "Not set")
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas sprawdzania trwa≈Ço≈õci danych: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/persistent-storage")
async def debug_persistent_storage():
    """Debug endpoint - sprawdza konfiguracjƒô trwa≈Çego katalogu danych"""
    try:
        import os
        
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Sprawd≈∫ zmienne ≈õrodowiskowe
        railway_volume_path = os.getenv("RAILWAY_VOLUME_PATH", "Not set")
        
        # Sprawd≈∫ katalogi
        data_dir = state_manager.data_dir
        channels_file = state_manager.channels_file
        quota_file = state_manager.quota_file
        system_file = state_manager.system_state_file
        
        # Sprawd≈∫ uprawnienia
        data_dir_writable = os.access(data_dir, os.W_OK) if data_dir.exists() else False
        channels_writable = os.access(channels_file.parent, os.W_OK) if channels_file.parent.exists() else False
        
        # Sprawd≈∫ czy pliki istniejƒÖ
        files_exist = {
            'channels.json': channels_file.exists(),
            'quota_state.json': quota_file.exists(),
            'system_state.json': system_file.exists()
        }
        
        # Sprawd≈∫ rozmiary plik√≥w
        file_sizes = {}
        for name, file_path in [('channels.json', channels_file), ('quota_state.json', quota_file), ('system_state.json', system_file)]:
            if file_path.exists():
                try:
                    file_sizes[name] = file_path.stat().st_size
                except Exception:
                    file_sizes[name] = -1
            else:
                file_sizes[name] = 0
        
        return {
            "environment": {
                "RAILWAY_VOLUME_PATH": railway_volume_path,
                "RAILWAY_ENVIRONMENT": os.getenv("RAILWAY_ENVIRONMENT", "Not set"),
                "PWD": os.getcwd()
            },
            "directories": {
                "data_directory": str(data_dir.absolute()),
                "data_directory_exists": data_dir.exists(),
                "data_directory_writable": data_dir_writable,
                "channels_directory_writable": channels_writable
            },
            "files": {
                "channels_file": str(channels_file.absolute()),
                "quota_file": str(quota_file.absolute()),
                "system_file": str(system_file.absolute()),
                "files_exist": files_exist,
                "file_sizes": file_sizes
            },
            "memory_state": {
                "channels_count": sum(len(channels) for channels in state_manager.channels_data.values()),
                "quota_used": state_manager.quota_state.get('used', 0),
                "system_startup": state_manager.system_state.get('last_startup')
            }
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania trwa≈Çego katalogu: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/channel-validation")
async def debug_channel_validation():
    """Debug endpoint - sprawdza walidacjƒô kana≈Ç√≥w i stan map"""
    try:
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Pobierz statystyki map
        maps_status = state_manager.get_channel_maps_status()
        
        # Pobierz dane kana≈Ç√≥w
        channels = state_manager.get_channels()
        
        # Sprawd≈∫ integralno≈õƒá danych
        validation_results = []
        total_channels = 0
        
        for category, category_channels in channels.items():
            category_validation = {
                'category': category,
                'channel_count': len(category_channels),
                'valid_channels': [],
                'invalid_channels': []
            }
            
            for i, channel in enumerate(category_channels):
                channel_id = channel.get('id', '')
                channel_name = channel.get('title', '')
                channel_url = channel.get('url', '')
                
                # Sprawd≈∫ walidacjƒô
                is_valid = True
                validation_errors = []
                
                if not channel_id or not channel_id.startswith('UC'):
                    validation_errors.append(f"Invalid channel_id: {channel_id}")
                    is_valid = False
                
                if not channel_name:
                    validation_errors.append("Missing channel_name")
                    is_valid = False
                
                if not channel_url:
                    validation_errors.append("Missing channel_url")
                    is_valid = False
                
                # Sprawd≈∫ czy jest w mapach
                in_id_map = channel_id in state_manager.channel_id_map
                in_url_map = channel_url in state_manager.channel_url_map
                
                if not in_id_map:
                    validation_errors.append("Not in channel_id_map")
                    is_valid = False
                
                if not in_url_map:
                    validation_errors.append("Not in channel_url_map")
                    is_valid = False
                
                channel_info = {
                    'index': i,
                    'channel_id': channel_id,
                    'channel_name': channel_name,
                    'channel_url': channel_url,
                    'in_id_map': in_id_map,
                    'in_url_map': in_url_map,
                    'validation_errors': validation_errors
                }
                
                if is_valid:
                    category_validation['valid_channels'].append(channel_info)
                else:
                    category_validation['invalid_channels'].append(channel_info)
                
                total_channels += 1
            
            validation_results.append(category_validation)
        
        return {
            "maps_status": maps_status,
            "validation_results": validation_results,
            "summary": {
                "total_channels": total_channels,
                "total_categories": len(channels),
                "maps_synchronized": maps_status['maps_synchronized']
            }
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania walidacji kana≈Ç√≥w: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.get("/debug/volume-config")
async def debug_volume_config():
    """Debug endpoint - sprawdza konfiguracjƒô /mnt/volume"""
    try:
        import os
        
        if not task_scheduler:
            raise HTTPException(status_code=500, detail="Scheduler nie jest dostƒôpny")
        
        state_manager = task_scheduler.state_manager
        
        # Sprawd≈∫ zmienne ≈õrodowiskowe
        railway_volume_path = os.getenv("RAILWAY_VOLUME_PATH", "Not set")
        
        # Sprawd≈∫ katalogi
        data_dir = state_manager.data_dir
        channels_file = state_manager.channels_file
        quota_file = state_manager.quota_file
        system_file = state_manager.system_state_file
        
        # Sprawd≈∫ uprawnienia
        data_dir_writable = os.access(data_dir, os.W_OK) if data_dir.exists() else False
        channels_writable = os.access(channels_file.parent, os.W_OK) if channels_file.parent.exists() else False
        
        # Sprawd≈∫ czy pliki istniejƒÖ
        files_exist = {
            'channels.json': channels_file.exists(),
            'quota_state.json': quota_file.exists(),
            'system_state.json': system_file.exists()
        }
        
        # Sprawd≈∫ rozmiary plik√≥w
        file_sizes = {}
        for name, file_path in [('channels.json', channels_file), ('quota_state.json', quota_file), ('system_state.json', system_file)]:
            if file_path.exists():
                try:
                    file_sizes[name] = file_path.stat().st_size
                except Exception:
                    file_sizes[name] = -1
            else:
                file_sizes[name] = 0
        
        # Sprawd≈∫ katalog /mnt/volume
        volume_dir = Path("/mnt/volume")
        volume_data_dir = volume_dir / "data"
        volume_reports_dir = volume_dir / "reports"
        
        return {
            "environment": {
                "RAILWAY_VOLUME_PATH": railway_volume_path,
                "RAILWAY_ENVIRONMENT": os.getenv("RAILWAY_ENVIRONMENT", "Not set"),
                "PWD": os.getcwd()
            },
            "volume_directories": {
                "/mnt/volume_exists": volume_dir.exists(),
                "/mnt/volume_writable": os.access(volume_dir, os.W_OK) if volume_dir.exists() else False,
                "/mnt/volume/data_exists": volume_data_dir.exists(),
                "/mnt/volume/data_writable": os.access(volume_data_dir, os.W_OK) if volume_data_dir.exists() else False,
                "/mnt/volume/reports_exists": volume_reports_dir.exists(),
                "/mnt/volume/reports_writable": os.access(volume_reports_dir, os.W_OK) if volume_reports_dir.exists() else False
            },
            "current_directories": {
                "data_directory": str(data_dir.absolute()),
                "data_directory_exists": data_dir.exists(),
                "data_directory_writable": data_dir_writable,
                "channels_directory_writable": channels_writable
            },
            "files": {
                "channels_file": str(channels_file.absolute()),
                "quota_file": str(quota_file.absolute()),
                "system_file": str(system_file.absolute()),
                "files_exist": files_exist,
                "file_sizes": file_sizes
            },
            "memory_state": {
                "channels_count": sum(len(channels) for channels in state_manager.channels_data.values()),
                "quota_used": state_manager.quota_state.get('used', 0),
                "system_startup": state_manager.system_state.get('last_startup')
            }
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas debugowania konfiguracji volume: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


# Endpoint backup usuniƒôty - plik backup_system.py przeniesiony do archiwum
# @router.post("/backup/create")
# async def create_system_backup():
#     """Tworzy pe≈Çny backup systemu"""
#     try:
#         import sys
#         import os
#         sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
#         from backup_system import BackupSystem
#         
#         backup_system = BackupSystem()
#         backup_path = backup_system.create_backup()
#         
#         # Zweryfikuj backup
#         verification_result = backup_system.verify_backup(backup_path)
#         
#         return {
#             "message": "Backup utworzony pomy≈õlnie",
#             "backup_path": backup_path,
#             "verification": "success" if verification_result else "failed",
#             "timestamp": datetime.now().isoformat()
#         }
#         
#     except Exception as e:
#         logger.error(f"B≈ÇƒÖd podczas tworzenia backupu: {e}")
#         raise HTTPException(status_code=500, detail=f"B≈ÇƒÖd podczas tworzenia backupu: {str(e)}") 


@router.post("/reports/rename-old-format")
async def rename_old_format_reports():
    """Przemianowuje stare raporty na nowy format nazewnictwa"""
    try:
        from ..storage.csv_generator import CSVGenerator
        csv_generator = CSVGenerator()
        result = csv_generator.rename_old_reports()
        
        print(f"üîÑ API: Przemianowanie zako≈Ñczone - {result['message']}")
        logger.info(f"Przemianowanie raport√≥w: {result['message']}")
        
        return {
            "success": True,
            "message": result['message'],
            "renamed_count": result['renamed'],
            "errors": result['errors'],
            "renamed_files": result.get('renamed_files', [])
        }
        
    except Exception as e:
        error_msg = f"B≈ÇƒÖd podczas przemianowania raport√≥w: {e}"
        print(f"‚ùå API: {error_msg}")
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg) 


@router.post("/trends/analyze-all")
async def analyze_all_csvs():
    """
    Endpoint do rƒôcznego uruchomienia analizy wszystkich istniejƒÖcych plik√≥w CSV.
    """
    try:
        logger.info("Rƒôczne uruchomienie analizy wszystkich plik√≥w CSV")
        
        # Sprawd≈∫ czy katalog raport√≥w istnieje
        reports_dir = "/mnt/volume/reports"
        if not os.path.exists(reports_dir):
            return {
                "message": "Katalog raport√≥w nie istnieje",
                "result": {
                    "total_processed": 0,
                    "total_success": 0,
                    "total_errors": 1,
                    "error": f"Katalog {reports_dir} nie istnieje"
                }
            }
        
        # Znajd≈∫ wszystkie pliki CSV
        csv_files = []
        for file in os.listdir(reports_dir):
            if file.endswith('.csv'):
                csv_files.append(file)
        
        logger.info(f"Znaleziono {len(csv_files)} plik√≥w CSV: {csv_files}")
        
        if not csv_files:
            return {
                "message": "Brak plik√≥w CSV do analizy",
                "result": {
                    "total_processed": 0,
                    "total_success": 0,
                    "total_errors": 0,
                    "files_found": []
                }
            }
        
        # Przetw√≥rz ka≈ºdy plik CSV
        processed_files = []
        successful_files = []
        errors = []
        
        for csv_file in csv_files:
            try:
                file_path = os.path.join(reports_dir, csv_file)
                logger.info(f"Przetwarzam plik: {csv_file}")
                
                # Wczytaj plik CSV
                import pandas as pd
                df = pd.read_csv(file_path)
                
                # Podstawowa analiza
                analysis_result = {
                    "filename": csv_file,
                    "rows": len(df),
                    "columns": list(df.columns),
                    "file_size": os.path.getsize(file_path),
                    "processed_at": "2025-08-24"
                }
                
                processed_files.append(analysis_result)
                successful_files.append(csv_file)
                logger.info(f"Pomy≈õlnie przetworzono: {csv_file} ({len(df)} wierszy)")
                
            except Exception as e:
                error_msg = f"B≈ÇƒÖd podczas przetwarzania {csv_file}: {str(e)}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        result = {
            "total_processed": len(processed_files),
            "total_success": len(successful_files),
            "total_errors": len(errors),
            "files_processed": processed_files,
            "files_successful": successful_files,
            "errors": errors
        }
        
        logger.info(f"Analiza zako≈Ñczona: {len(processed_files)} przetworzonych, {len(successful_files)} pomy≈õlnie")
        
        return {
            "message": "Analiza wszystkich plik√≥w CSV zako≈Ñczona",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas analizy wszystkich CSV: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/trends/{category_name}/reanalyze")
async def reanalyze_category(category_name: str, date: str = None):
    """
    Endpoint do wymuszenia ponownej analizy kategorii.
    """
    try:
        logger.info(f"Wymuszenie ponownej analizy dla kategorii {category_name} {date or 'dzisiaj'}")
        
        # Sprawd≈∫ czy katalog raport√≥w istnieje
        reports_dir = "/mnt/volume/reports"
        if not os.path.exists(reports_dir):
            raise HTTPException(
                status_code=400, 
                detail=f"Katalog raport√≥w {reports_dir} nie istnieje"
            )
        
        # Znajd≈∫ pliki CSV dla konkretnej kategorii
        category_pattern = f"report_{category_name.upper()}_*.csv"
        csv_files = []
        for file in os.listdir(reports_dir):
            if file.startswith(f"report_{category_name.upper()}_") and file.endswith('.csv'):
                csv_files.append(file)
        
        if not csv_files:
            return {
                "message": f"Brak plik√≥w CSV dla kategorii {category_name}",
                "category": category_name,
                "date": date or "dzisiaj",
                "files_found": []
            }
        
        # Przetw√≥rz pliki CSV dla kategorii
        processed_files = []
        for csv_file in csv_files:
            try:
                file_path = os.path.join(reports_dir, csv_file)
                logger.info(f"Przetwarzam plik kategorii: {csv_file}")
                
                # Wczytaj plik CSV
                import pandas as pd
                df = pd.read_csv(file_path)
                
                # Analiza dla kategorii
                analysis_result = {
                    "filename": csv_file,
                    "category": category_name,
                    "rows": len(df),
                    "columns": list(df.columns),
                    "file_size": os.path.getsize(file_path),
                    "processed_at": "2025-08-24"
                }
                
                processed_files.append(analysis_result)
                logger.info(f"Pomy≈õlnie przeanalizowano {category_name}: {csv_file} ({len(df)} wierszy)")
                
            except Exception as e:
                logger.error(f"B≈ÇƒÖd podczas analizy {category_name} {csv_file}: {e}")
                raise HTTPException(
                    status_code=400, 
                    detail=f"Nie uda≈Ço siƒô przeanalizowaƒá {category_name}: {str(e)}"
                )
        
        return {
            "message": f"Pomy≈õlnie przeanalizowano {category_name}",
            "category": category_name,
            "date": date or "dzisiaj",
            "files_processed": processed_files,
            "total_files": len(processed_files)
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas ponownej analizy {category_name}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/trends/local-analysis/{category_name}")
async def get_local_trend_analysis(category_name: str):
    """
    Endpoint do wy≈õwietlania wynik√≥w lokalnej analizy trend√≥w.
    Wczytuje plik JSON wygenerowany przez lokalny analizator.
    """
    try:
        logger.info(f"Pobieranie lokalnej analizy trend√≥w dla kategorii {category_name}")
        
        # ≈öcie≈ºka do pliku JSON z lokalnej analizy
        json_file_path = f"/mnt/volume/reports/trend_analysis_{category_name.lower()}_latest.json"
        
        if not os.path.exists(json_file_path):
            return {
                "error": f"Brak lokalnej analizy dla kategorii {category_name}",
                "message": "Uruchom lokalny analizator ≈ºeby wygenerowaƒá analizƒô",
                "file_path": json_file_path
            }
        
        # Wczytaj plik JSON
        with open(json_file_path, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        logger.info(f"Pomy≈õlnie wczytano lokalnƒÖ analizƒô: {len(analysis_data.get('reports', []))} raport√≥w")
        
        return {
            "message": f"Lokalna analiza trend√≥w dla {category_name}",
            "data": analysis_data
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas wczytywania lokalnej analizy dla {category_name}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/test-trend-endpoint")
async def test_trend_endpoint():
    """
    Test endpoint ≈ºeby sprawdziƒá czy nasze zmiany zosta≈Çy wdro≈ºone.
    """
    return {
        "message": "‚úÖ Trend endpoint dzia≈Ça!",
        "timestamp": "2025-08-24",
        "status": "working"
    }


@router.post("/trends/upload-analysis")
async def upload_trend_analysis(category_name: str, file: UploadFile = File(...)):
    """
    Endpoint do przesy≈Çania plik√≥w JSON z analizƒÖ trend√≥w.
    """
    try:
        logger.info(f"Przesy≈Çanie analizy trend√≥w dla kategorii {category_name}")
        
        # Sprawd≈∫ czy plik to JSON
        if not file.filename.endswith('.json'):
            raise HTTPException(
                status_code=400, 
                detail="Plik musi byƒá w formacie JSON"
            )
        
        # Sprawd≈∫ czy katalog reports istnieje
        reports_dir = "/mnt/volume/reports"
        if not os.path.exists(reports_dir):
            os.makedirs(reports_dir, exist_ok=True)
            logger.info(f"Utworzono katalog: {reports_dir}")
        
        # Zapisz plik
        file_path = os.path.join(reports_dir, f"trend_analysis_{category_name.lower()}_latest.json")
        
        with open(file_path, 'wb') as f:
            content = await file.read()
            f.write(content)
        
        logger.info(f"Pomy≈õlnie zapisano analizƒô trend√≥w: {file_path}")
        
        return {
            "message": f"Analiza trend√≥w dla {category_name} zosta≈Ça przes≈Çana",
            "file_path": file_path,
            "file_size": len(content),
            "category": category_name
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas przesy≈Çania analizy trend√≥w: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 


@router.post("/force-report/{category}")
async def force_report_generation(category: str):
    """
    Wymusza wygenerowanie raportu dla danej kategorii.
    U≈ºyteczne do testowania i debugowania.
    """
    try:
        logger.info(f"Wymuszam generowanie raportu dla kategorii: {category}")
        
        # Pobierz kana≈Çy dla danej kategorii
        channels = task_scheduler.get_channels().get(category, [])
        
        if not channels:
            return {"detail": f"Nie znaleziono kana≈Ç√≥w dla kategorii {category}"}
        
        # Pobierz dane z YouTube API
        all_videos = []
        for channel in channels:
            try:
                videos = await task_scheduler.get_channel_videos(
                    channel['id'], 
                    settings.days_back
                )
                
                # Dodaj informacje o kanale do ka≈ºdego filmu
                for video in videos:
                    video['channel_title'] = channel['title']
                    video['channel_id'] = channel['id']
                
                all_videos.extend(videos)
                logger.info(f"Pobrano {len(videos)} film√≥w z kana≈Çu {channel['title']}")
                
            except Exception as e:
                logger.error(f"B≈ÇƒÖd podczas pobierania film√≥w z kana≈Çu {channel['title']}: {e}")
        
        if not all_videos:
            return {"detail": f"Nie uda≈Ço siƒô pobraƒá film√≥w dla kategorii {category}"}
        
        # Generuj raport CSV
        csv_generator = CSVGenerator()
        csv_path = csv_generator.generate_csv(all_videos, category)
        logger.info(f"Wygenerowano raport dla kategorii {category}: {csv_path}")
        
        # Generuj ranking (je≈õli modu≈Ç trend√≥w jest aktywny)
        ranking_path = None
        if os.environ.get("ENABLE_TREND", "false").lower() == "true":
            try:
                from app.trend.services.ranking_manager import ranking_manager
                import pandas as pd
                
                # Wczytaj dane z wygenerowanego CSV
                df = pd.read_csv(csv_path)
                csv_videos = df.to_dict('records')
                
                # Aktualizuj ranking u≈ºywajƒÖc danych z CSV
                ranking = ranking_manager.update_ranking(category, csv_videos)
                ranking_path = f"data/rankings/ranking_{category.upper()}.json"
                logger.info(f"Wygenerowano ranking dla kategorii {category}")
            except Exception as e:
                logger.warning(f"Nie uda≈Ço siƒô wygenerowaƒá rankingu: {e}")
        
        return {
            "message": f"Raport dla kategorii {category} zosta≈Ç wygenerowany pomy≈õlnie",
            "csv_path": str(csv_path),
            "ranking_path": ranking_path,
            "videos_count": len(all_videos),
            "channels_count": len(channels)
        }
        
    except Exception as e:
        logger.error(f"B≈ÇƒÖd podczas wymuszonego generowania raportu dla {category}: {e}")
        return {"detail": f"B≈ÇƒÖd podczas generowania raportu: {str(e)}"} 